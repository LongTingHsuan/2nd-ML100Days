{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Day_100_transfer_learning_HW.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Fu6vb64bokvm","colab_type":"text"},"source":["## 作業\n","礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n","\n","\n","最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n","\n","另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"]},{"cell_type":"code","metadata":{"id":"6Ns4tWhzokvo","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":71},"outputId":"8d739707-ae0e-4ca9-9ad9-f6949d18e475","executionInfo":{"status":"ok","timestamp":1565544298072,"user_tz":-480,"elapsed":9319,"user":{"displayName":"Tom Long","photoUrl":"","userId":"18258624704255876647"}}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-90cad39b-5c06-4ee8-8837-bfedcfc7b316\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-90cad39b-5c06-4ee8-8837-bfedcfc7b316\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving resnet_builder.py to resnet_builder.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"beaSL2tFsmKd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2b6f6f3f-526d-4332-c393-69b4b936cefd","executionInfo":{"status":"ok","timestamp":1565544321521,"user_tz":-480,"elapsed":2427,"user":{"displayName":"Tom Long","photoUrl":"","userId":"18258624704255876647"}}},"source":["from keras.datasets import cifar10\n","from resnet_builder import resnet # 這是從 resnet_builder.py 中直接 import 撰寫好的 resnet 函數\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2iKCBuJ4tog6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"fdd398d5-0506-4ee1-b632-b78cc4834abe","executionInfo":{"status":"ok","timestamp":1565544331120,"user_tz":-480,"elapsed":1988,"user":{"displayName":"Tom Long","photoUrl":"","userId":"18258624704255876647"}}},"source":["# 讀取資料集並作前處理\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train = x_train / 255.\n","x_test = x_test / 255.\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZbuWSSmytq9W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8b2fd691-4589-43d4-a6b5-108b68eca10a","executionInfo":{"status":"ok","timestamp":1565544341388,"user_tz":-480,"elapsed":4181,"user":{"displayName":"Tom Long","photoUrl":"","userId":"18258624704255876647"}}},"source":["# 建立 ResNet 模型\n","model = resnet(input_shape=(32,32,3)) \n","model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0811 17:25:37.478921 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0811 17:25:37.499078 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0811 17:25:37.505666 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","W0811 17:25:37.546579 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0811 17:25:37.550244 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0811 17:25:38.180103 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0811 17:25:40.739974 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n","                                                                 conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 32, 16)   1040        activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 32, 64)   1088        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 16, 16, 64)   4160        activation_10[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n","                                                                 conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 16, 16, 64)   8256        activation_13[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 128)  8320        activation_15[0][0]              \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n","                                                                 conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 16, 16, 64)   8256        activation_16[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 16, 16, 128)  8320        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n","                                                                 conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 8, 8, 128)    16512       activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 8, 8, 256)    33024       add_6[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       activation_21[0][0]              \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_25[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 8, 8, 128)    32896       activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 8, 8, 256)    33024       activation_24[0][0]              \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n","                                                                 conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 8, 8, 128)    32896       activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 8, 8, 256)    33024       activation_27[0][0]              \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n","                                                                 conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_28[0][0]              \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n","==================================================================================================\n","Total params: 849,002\n","Trainable params: 843,786\n","Non-trainable params: 5,216\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ybuH_DtGts7_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ef4d63f9-79da-4635-abf4-a3b8708583fb","executionInfo":{"status":"ok","timestamp":1565545606228,"user_tz":-480,"elapsed":1234682,"user":{"displayName":"Tom Long","photoUrl":"","userId":"18258624704255876647"}}},"source":["batch_size = 64 # batch 的大小，如果出現 OOM error，請降低這個值\n","num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n","epochs = 30 # 訓練整個資料集共 30個循環\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(),\n","              metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["W0811 17:26:11.824206 140396774508416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0811 17:26:12.468766 140396774508416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/30\n","50000/50000 [==============================] - 48s 969us/step - loss: 1.7885 - acc: 0.5283 - val_loss: 1.7505 - val_acc: 0.5266\n","Epoch 2/30\n","50000/50000 [==============================] - 40s 807us/step - loss: 1.3064 - acc: 0.6655 - val_loss: 1.3403 - val_acc: 0.6390\n","Epoch 3/30\n","50000/50000 [==============================] - 41s 811us/step - loss: 1.1071 - acc: 0.7250 - val_loss: 1.1587 - val_acc: 0.7014\n","Epoch 4/30\n","50000/50000 [==============================] - 41s 812us/step - loss: 0.9657 - acc: 0.7690 - val_loss: 1.2271 - val_acc: 0.6876\n","Epoch 5/30\n","50000/50000 [==============================] - 41s 815us/step - loss: 0.8799 - acc: 0.7970 - val_loss: 1.1854 - val_acc: 0.6941\n","Epoch 6/30\n","50000/50000 [==============================] - 41s 814us/step - loss: 0.8029 - acc: 0.8245 - val_loss: 1.2290 - val_acc: 0.6838\n","Epoch 7/30\n","50000/50000 [==============================] - 41s 817us/step - loss: 0.7574 - acc: 0.8388 - val_loss: 1.2725 - val_acc: 0.6840\n","Epoch 8/30\n","50000/50000 [==============================] - 41s 817us/step - loss: 0.7110 - acc: 0.8556 - val_loss: 1.1059 - val_acc: 0.7341\n","Epoch 9/30\n","50000/50000 [==============================] - 41s 818us/step - loss: 0.6704 - acc: 0.8694 - val_loss: 1.4009 - val_acc: 0.6853\n","Epoch 10/30\n","50000/50000 [==============================] - 41s 816us/step - loss: 0.6352 - acc: 0.8819 - val_loss: 1.0982 - val_acc: 0.7377\n","Epoch 11/30\n","50000/50000 [==============================] - 41s 815us/step - loss: 0.6058 - acc: 0.8936 - val_loss: 1.3297 - val_acc: 0.7232\n","Epoch 12/30\n","50000/50000 [==============================] - 41s 813us/step - loss: 0.5897 - acc: 0.9002 - val_loss: 1.1264 - val_acc: 0.7517\n","Epoch 13/30\n","50000/50000 [==============================] - 41s 817us/step - loss: 0.5685 - acc: 0.9076 - val_loss: 1.3122 - val_acc: 0.7146\n","Epoch 14/30\n","50000/50000 [==============================] - 41s 812us/step - loss: 0.5576 - acc: 0.9116 - val_loss: 1.7078 - val_acc: 0.6429\n","Epoch 15/30\n","50000/50000 [==============================] - 41s 813us/step - loss: 0.5401 - acc: 0.9175 - val_loss: 1.2106 - val_acc: 0.7494\n","Epoch 16/30\n","50000/50000 [==============================] - 41s 810us/step - loss: 0.5305 - acc: 0.9223 - val_loss: 1.3954 - val_acc: 0.7251\n","Epoch 17/30\n","50000/50000 [==============================] - 40s 809us/step - loss: 0.5204 - acc: 0.9263 - val_loss: 1.3169 - val_acc: 0.7234\n","Epoch 18/30\n","50000/50000 [==============================] - 40s 806us/step - loss: 0.5199 - acc: 0.9268 - val_loss: 1.4088 - val_acc: 0.7278\n","Epoch 19/30\n","50000/50000 [==============================] - 41s 810us/step - loss: 0.5099 - acc: 0.9306 - val_loss: 1.4561 - val_acc: 0.7306\n","Epoch 20/30\n","50000/50000 [==============================] - 40s 808us/step - loss: 0.5068 - acc: 0.9325 - val_loss: 1.3989 - val_acc: 0.7312\n","Epoch 21/30\n","50000/50000 [==============================] - 41s 818us/step - loss: 0.4887 - acc: 0.9392 - val_loss: 1.8901 - val_acc: 0.6779\n","Epoch 22/30\n","50000/50000 [==============================] - 41s 816us/step - loss: 0.4919 - acc: 0.9370 - val_loss: 1.7425 - val_acc: 0.6880\n","Epoch 23/30\n","50000/50000 [==============================] - 41s 812us/step - loss: 0.4902 - acc: 0.9380 - val_loss: 1.4908 - val_acc: 0.7414\n","Epoch 24/30\n","50000/50000 [==============================] - 41s 811us/step - loss: 0.4833 - acc: 0.9404 - val_loss: 1.6440 - val_acc: 0.7129\n","Epoch 25/30\n","50000/50000 [==============================] - 41s 810us/step - loss: 0.4745 - acc: 0.9440 - val_loss: 1.4136 - val_acc: 0.7394\n","Epoch 26/30\n","50000/50000 [==============================] - 40s 810us/step - loss: 0.4799 - acc: 0.9415 - val_loss: 1.2450 - val_acc: 0.7545\n","Epoch 27/30\n","50000/50000 [==============================] - 40s 807us/step - loss: 0.4716 - acc: 0.9449 - val_loss: 1.4471 - val_acc: 0.7390\n","Epoch 28/30\n","50000/50000 [==============================] - 41s 810us/step - loss: 0.4702 - acc: 0.9458 - val_loss: 1.6448 - val_acc: 0.7060\n","Epoch 29/30\n","50000/50000 [==============================] - 40s 807us/step - loss: 0.4672 - acc: 0.9469 - val_loss: 1.8770 - val_acc: 0.6789\n","Epoch 30/30\n","50000/50000 [==============================] - 41s 811us/step - loss: 0.4726 - acc: 0.9454 - val_loss: 1.3092 - val_acc: 0.7475\n","Test loss: 1.3091973957061767\n","Test accuracy: 0.7475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BmM_TjfGt1Ue","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}